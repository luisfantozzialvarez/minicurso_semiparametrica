% !TeX document-id = {0be8c18c-9430-4e9a-bdd9-12beadebfebc}
% !TeX TXS-program:bibliography = txs:///biber
\documentclass[11pt]{beamer}
\uselanguage{portuguese}
\languagepath{portuguese}
\deftranslation[to=portuguese]{Theorem}{Teorema}
\deftranslation[to=portuguese]{theorem}{teorema}
\deftranslation[to=portuguese]{Example}{Exemplo}
\deftranslation[to=portuguese]{example}{exemplo}
\deftranslation[to=portuguese]{Lemma}{Lema}
\deftranslation[to=portuguese]{lemma}{Lema}
\deftranslation[to=portuguese]{Corollary}{Corolário}
\deftranslation[to=portuguese]{corollary}{corolário}
%\deftranslation[to=portuguese]{and}{e}

\usepackage[brazilian]{babel}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{lmodern}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{color}
\usepackage{pgfplots}
\usepackage{tikz}

%\usepackage{appendixnumberbeamer}

\newenvironment{transitionframe}{
	\setbeamercolor{background canvas}{bg=yellow}
	\begin{frame}}{
	\end{frame}
}
\usetheme{default}
\usefonttheme{structuresmallcapsserif}

%% I use a beige off white for my background
\definecolor{MyBackground}{RGB}{255,253,218}
\useinnertheme[shadow]{rounded}
\setbeamercolor{block title}{bg=MyBackground}
\setbeamercolor{block body}{bg=MyBackground}
\setbeamercolor{example title}{bg=MyBackground}
\setbeamercolor{example body}{bg=MyBackground}


\newcommand{\blue}[1]{\textcolor{blue}{#1}}
\newcommand{\red}[1]{\textcolor{red}{#1}}
\newcommand{\purple}[1]{\textcolor{purple}{#1}}
\newcommand{\gray}[1]{\textcolor{gray}{#1}}
\setbeamertemplate{navigation symbols}{}
%\setbeamertemplate{page number in head/foot}[appendixframenumber]

%\usepackage{graphics}
\usepackage{graphicx}

\definecolor{blue_emph}{RGB}{0,114,178}
\definecolor{red}{RGB}{213,94,0}
\definecolor{yellow}{RGB}{240,228,66}
\definecolor{green}{RGB}{0,158,115}
\definecolor{purple}{RGB}{204,121,167}
\definecolor{orange}{RGB}{230,159,0}
\definecolor{lightblue}{RGB}{86,180,233}

%\setbeamercolor{frametitle}{fg=blue}
%\setbeamercolor{title}{fg=blue}
\setbeamertemplate{footline}[frame number]
\setbeamertemplate{navigation symbols}{} 
\setbeamertemplate{itemize items}{-}
%\setbeamercolor{itemize item}{fg=blue}
%\setbeamercolor{itemize subitem}{fg=blue}
\setbeamertemplate{enumerate items}[default]
%\setbeamercolor{enumerate subitem}{fg=blue}
\setbeamercolor{button}{bg=MyBackground,fg=blue}
\usefonttheme{structuresmallcapsserif}

%\setbeamercolor{section in toc}{fg=blue}
%\setbeamercolor{subsection in toc}{fg=red}
\setbeamersize{text margin left=1em,text margin right=1em} 


\usepackage{appendixnumberbeamer}

\usepackage[
backend=biber,
style=authoryear,
natbib=true
]{biblatex}
\addbibresource{../bibliography.bib}

\newenvironment{wideitemize}{\itemize\addtolength{\itemsep}{10pt}}{\enditemize}
\newenvironment{wideenumerate}{\enumerate\addtolength{\itemsep}{10pt}}{\endenumerate}
\newenvironment{halfwideitemize}{\itemize\addtolength{\itemsep}{0.5em}}{\enditemize}
\newenvironment{halfwideenumerate}{\enumerate\addtolength{\itemsep}{0.5em}}{\endenumerate}


\author{Luis A. F. Alvarez}
\title{Introdução à Econometria Semiparamétrica}
\subtitle{Aula 1 - Estimação Não Paramétrica Clássica}
%\logo{}
%\institute{}
\date{\today}
%\subject{}
%\setbeamercovered{transparent}

\begin{document}

	\begin{frame}[plain]
	\maketitle
	\end{frame}
	\begin{frame}{Estimando uma densidade}
		\begin{halfwideitemize}
			\item Suponha que você, pesquisador, possui uma amostra aleatória $\{X_i\}_{i=1}^n$ de uma distribuição contínua $F$, cuja densidade denotamos por $f$.
			\begin{itemize}
				\item Por exemplo, você possui dados da riqueza não financeira de uma amostra de indivíduos.
			\end{itemize}
			\item Você está interessado em estimar a densidade $f$.
			\item Como você pode fazer isso?
		\end{halfwideitemize}
	\end{frame}
	
	\begin{frame}{Estimação paramétrica}
		\begin{itemize}
			\item Um caminho consiste em realizar uma {\color{blue}hipótese de forma funcional} acerca de $f$.
			\item Por exemplo, se nossa população de interesse está restrita à cauda superior da distribuição de riqueza, a literatura sugere que deve valer a ``lei de Pareto'' \citep{benhabib2018skewed}, segundo a qual:
			$$f(x) = g(x|x_0,\alpha) = \alpha \frac{x_0^{\alpha}}{x^{\alpha+1}}\mathbf{1}\{x\geq x_0\}, \quad x_0,\alpha>0$$
			 \begin{itemize}
			 	\item Distribuição compatível com modelos de agentes heterogêneos com mercados incompletos e ativos arriscados \citep{achdou2022income}.
			 \end{itemize}
			 \item Sob a hipótese de forma funcional, a estimação de $f$ se resume à estimação de dois parâmetros, $(x_0,\alpha)\in \mathbb{R}^2$.
			 \item Por exemplo, podemos estimá-los através de máxima verossimilhança:
			 $$(\hat x_0, \hat \alpha) \in \operatorname{argmax}_{(z,a)\in \mathbb{R}^2} \frac{1}{n}\sum_{i=1}^n \log g(X_i|z,a)$$
		\end{itemize}
	\end{frame}
	\begin{frame}{Forma funcional e estimação não paramétrica}
		\begin{itemize}
			\item E se a lei de Pareto não for válida?
			\begin{itemize}
				\item Nesse caso, a densidade estimada não recuperará a distribuição correta, mesmo em amostras grandes.
			\end{itemize}
			\item Como podemos estimar a distribuição correta sem requerer a hipótese de forma funcional?
			\item Para isso, necessitamos de métodos {\color{blue}não paramétricos}, que dispensem da hipótese de forma funcional.
		\end{itemize}
	\end{frame}
	\begin{frame}{Histograma}
		\begin{itemize}
			\item O estimador não paramétrico mais simples é o histograma.
			\item Em um histograma, o suporte da distribuição é dividio em $J$ células (\textit{bins}) de comprimento $2h$, e a densidade para os pontos na $j$-ésima célula, $[a_j,b_j)$, é estimada como constante igual a:
			
			$$\hat{f}_h(x_j) = \frac{1}{n2h}\sum_{i=1}^n \mathbf{1}\left\{|X_i-x_j|\leq h\right\}\,,$$ 
			onde o $x_j = \frac{a_j+b_j}{2}$  é o ponto intermediário da célula $j$.
		\end{itemize}
	\end{frame}
	\begin{frame}{Estimador de \textit{kernel} para densidade}
		\begin{itemize}
			\item O estimador $\hat{f}_h(x_j)$ usado na construção do histograma é um caso particular de um {\color{blue}estimador de \textit{kernel}} para uma densidade em um ponto $x$, que é dado por:
			
			$$\hat{f}_h(x) = \frac{1}{nh}\sum_{j=1}^n K\left(\frac{X_j - x}{h}\right) \, ,$$
			onde $K:\mathbb{R}\mapsto \mathbb{R}$ é um \textit{kernel}, i.e. uma função satisfazendo $\int_{-\infty}^\infty K(u) du = 1$ e simétrica em torno do zero, $K(u) = K(-u)$.
			\item No que segue, vamos supor que $K$ é uma função não negativa com suporte no intervalo $[-1,1]$, embora essas hipóteses não sejam necessárias para a maioria dos resultados.
			\item Sob a hipótese anterior, estimador de \textit{kernel} propõe estimar $f(x)$ ``contando'' os pontos da amostra com distância a menos de $h$ de $x$. $K$ nos dá então o peso relativo de cada ponto dentro de essa região.
		\end{itemize}
	\end{frame}
	\begin{frame}{Exemplos de \textit{kernel}}
		\begin{table}[h!]
			\centering
			\begin{tabular}{|c|c|}
				\hline
				\textbf{\textit{Kernel}}         & \textbf{Fórmula}                                      \\ \hline
				\textbf{Uniforme}        & $K(u) = \frac{1}{2} \cdot \mathbf{1}(|u| \leq 1)$     \\ \hline
				\textbf{Triangular}     & $K(u) = (1 - |u|) \cdot \mathbf{1}(|u| \leq 1)$       \\ \hline
				\textbf{Epanechnikov}   & $K(u) = \frac{3}{4}(1 - u^2) \cdot \mathbf{1}(|u| \leq 1)$ \\ \hline
			\end{tabular}
		\end{table}
	
	\end{frame}
	\begin{frame}{Exemplos de \textit{kernel}}
		\centering 
			\begin{tikzpicture}
			\begin{axis}[
				title={Funções \textit{Kernel}},
				xlabel={$u$},
				ylabel={$K(u)$},
				grid=major,
				legend pos=north west,
				 legend style={font=\fontsize{6}{6}\selectfont},
				axis lines=middle,
				xmin=-1.5, xmax=1.5, 
				ymin=0, ymax=1,
				samples=100
				]
				% Uniform Kernel
				\addplot[domain=-1:1, blue, thick] {0.5};
				\addlegendentry{Uniforme}
				
				% Triangular Kernel
				\addplot[domain=-1:1, green, thick] {1 - abs(x)};
				\addlegendentry{Triangular}
				
				% Epanechnikov Kernel
				\addplot[domain=-1:1, red, thick] {0.75*(1 - x^2)};
				\addlegendentry{Epanechnikov}
				
			\end{axis}
		\end{tikzpicture}
	\end{frame}
	
	\begin{frame}{Calculando o viés do estimador de \textit{kernel}}
		\begin{itemize}
			\item Vamos analisar as propriedades estatísticas do estimador de \textit{kernel}.
			
			\item Considere um ponto $x$ do suporte da distribuição tal que $f(\cdot)$ seja duas vezes continuamente diferenciável numa vizinhança de $x$. Neste caso, temos que, à medida que $h \to 0$:
			\begin{equation*}
				\begin{aligned}
					\mathbb{E}[\hat{f}_h(x)] = \frac{1}{nh} \sum_{i=1}^n\mathbb{E}\left[K\left(\frac{X_i - x}{h}\right)\right] = \frac{1}{h}\mathbb{E}\left[K\left(\frac{X_1 - x}{h}\right)\right] = \\
				 \frac{1}{h}\int_{x - h}^{x+h} K\left(\frac{s-x}{h}\right) f(s) ds \overset{u = \frac{s-x}{h}}{=}	\int_{-1}^{1} K(u) f(x+hu) du = \\
					f(x)\underbrace{\int_{-1}^{-1}K(u)du}_{=1} + f'(x) h \underbrace{\int_{-1}^{-1}uK(u)du}_{=0} + \int_{-1}^1 (uh)^2 f''(\tilde{x}_{hu}) K(u) du = \\
					f(x) +h^2 f''(x) \int_{-1}^{1} u^2 K(u) du + o(h^2)  
				\end{aligned}
			\end{equation*}
			onde, da segunda para a terceira linha, aplicamos o teorema do valor médio de segunda ordem, e a última passagem usa continuidade (uniforme) da segunda derivada numa vizinhança de $x$.
		\end{itemize}
	\end{frame}
	
	\begin{frame}{Calculando a variância do estimador de \textit{kernel}}
		\begin{itemize}
			\item Do mesmo modo:
			$$		\mathbb{V}[\hat{f}_h(x)] = \frac{1}{nh^2}\mathbb{V}\left[K\left(\frac{X_1 - X}{h}\right)\right]$$
			\item Mas:
			$$\mathbb{E}\left[K\left(\frac{X_1 - X}{h}\right)^2\right] = h \int_{-1}^{1} f(x) K^2(u) du + O(h^3)$$
			e$$\mathbb{E}\left[K\left(\frac{X_1 - X}{h}\right)\right]^2 = h^2f(x)^2  + O(h^3)$$
			\item De modo que:
			$$\mathbb{V}[\hat{f}_h(x)] = \frac{1}{nh} \int_{-1}^{1} f(x) K^2(u) du+ \frac{1}{n}f(x)^2 + O\left(\frac{h}{n}\right)$$		
		\end{itemize}
		

	\end{frame}
	\begin{frame}{Erro quadrático médio do estimador de \textit{kernel}}
		\begin{itemize}
			\item Combinando os pontos anteriores, temos que:
			\begin{equation*}
				\small 
				\begin{aligned}
					\operatorname{EQM}[\hat{f}_h(x)] = \mathbb{E}[(\hat{f}_h(x) - f(x))^2] = \\ 
					h^4 f''(x)^2 \left(\int_{-1}^{1}u^2 K(u)du\right)^2 + \frac{1}{nh} \int_{-1}^{1} f(x) K^2(u) du+ \frac{1}{n}f(x)^2 + o\left(\frac{1}{nh} \lor h^4\right)
				\end{aligned}
			\end{equation*}
			\item Portanto, se $n \to \infty$ com $h\to 0$ e $nh \to \infty$.
			$$\operatorname{EQM}[\hat{f}_h(x)] \to 0\, ,$$
			e o estimador de kernel é \textit{consistente}.
			\item Além disso, da expressão acima escolha $h$ ótima, que balanceia \textit{trade-off} viés-variância,  é dada por:
			$$h_n \propto n^{-1/5} $$ 
			
		\end{itemize}
	\end{frame}
	\begin{frame}{Estimação de densidades multidimensionais}
		\begin{itemize}
			\item O estimador anteriormente introduzido é facilmente extensível para a estimação de uma densidade em $\mathbb{R}^d$. Seja $\boldsymbol{f}$ uma densidade em $\mathbb{R}^d$, e $\{\boldsymbol{X}_i\}_{i=1}^n$ uma amostra aleatória desta distribuição. Podemos estimar $\boldsymbol{f}(\boldsymbol{x})$ como:
			
			$$\hat{\boldsymbol{f}}_{\boldsymbol{h}}(x) = \frac{1}{nh_1\ldots h_d}\sum_{i=1}^n \boldsymbol{K}_{\boldsymbol{h}}\left(\boldsymbol{X}_i - \boldsymbol{x}\right)$$
			onde $\boldsymbol{K}_{\boldsymbol{h}}(\boldsymbol{u}) =\prod_{j=1}^d K(u_j/h_j)$, com $K$ um \textit{kernel} univariado.
			\item Nesse caso, viés é proporcional da ordem de $d \max_j h^2_j$ e variância a $\frac{1}{nh_1\ldots h_d}$. Portanto, se:
			
			$$\max_j h_j \to 0,\quad  nh_1\ldots h_d \to \infty \, ,$$
			estimador é consistente.
		\end{itemize}
	\end{frame}
	
	\begin{frame}{Estimador Nadaraya-Watson para a esperança condicional}
		\begin{itemize}
			\item A lógica do estimador anterior pode ser estendida para, dada uma amostra aleatória $\{(Y_i,X_i)\}_{i=1}^n$ de  $(Y,X)$, estimar-se a função de esperança condicional $m(X) = \mathbb{E}[Y|X=x]$ num ponto ${x}$.
			\item Nesse caso, podemos tomar médias locais, dadas pelo estimador de {\color{blue}Nadaraya-Watson}:
			$$\small \hat{m}(x) = \frac{\sum_{i=1}^n K\left( (X_i-x)/h\right) Y_i}{\sum_{i=1}^n K\left((X_i-x)/h \right)}$$
			
		\end{itemize}
	\end{frame}
	\begin{frame}{Viés e variância do estimador de Nadaraya-Watson}
		\begin{itemize}
			\item 	Para uma $m$ duas vezes continuamente diferenciável numa vizinhança de ${x}$, com densidade $f$ positiva e continuamente diferenciável numa vizinhança de ${x}$, temos, se $n\to \infty$ e $nh\to \infty$
			$$\small \text{Vi\'{e}s}(\hat{m}(x)) = \left(\frac{1}{2} m^{\prime \prime}(x)+\frac{m^{\prime}(x) f^{\prime}(x)}{f(x)}\right) \int_{-\infty}^{\infty} u^{2} K(u) d u h^{2} + o(h^2)$$
			$$\mathbb{V}(\hat{m}(x)) = \frac{\sigma^{2}(x)}{f(x) n h} \int_{-\infty}^{\infty} K^{2}(u) d u + o\left(\frac{1}{nh}\right)$$
			onde $\sigma^2(X) = \mathbb{V}[Y|X]$.
			\item Estimador é consistente para $m(x)$ se $n\to \infty$, $h\to 0$ e $nh\to \infty$.
			\item Banda ótima em termos de erro quadrático médio é dada por:
			$h_n \propto n^{-1/5}$
		\end{itemize}

	\end{frame}

	
	
	\begin{frame}{Viés de fronteira}
	\begin{itemize}
		\item A derivação anterior pressupõe um ponto interior ao suporte de $X$,
		\item Para pontos na fronteira do suporte, a expansão de Taylor é unilateral, de modo que haverá um componente de viés adicional em $\hat{m}(x)$.
		\begin{itemize}
			\item Esse componente adicional de viés tem taxa de convergência mais lenta, de $h$ e não $h^2$, e será proporcional a $m'(x)$.
		\end{itemize}
	\end{itemize}
	\end{frame}
	
	\begin{frame}{Visualização}

    \begin{tikzpicture}
	\begin{axis}[
		width=10cm, height=7cm,
		xlabel={$x$},
		ylabel={$y$},
		grid=both,
		xmin=0, xmax=10,
		ymin=0, ymax=3,
		legend pos=south east,
		]
		
		% Simulated "true" function (centered around data)
    \addplot[domain=0:10, smooth, thick, blue] {1.5 + sin(deg(x))};

		
		
		% Boundary area
		\draw [fill=gray!30,draw=none] (0,0) rectangle (1,3);
		%\node at (axis cs: 0.5, 2.5) {Boundary Bias};
		
		% Data points (sample, centered around true function)
\addplot[only marks, mark=*, black] coordinates {
	(0.25, 1.7)
	(0.5, 1.5 + 0.479425 + 0.1) 
	(1, 1.5 + 0.841471 - 0.2)
	(1.5, 1.5 + 0.997495 + 0.05) 
	(2, 1.5 + 0.909297 + 0.15)
	(2.5, 1.5 + 0.598472 - 0.1) 
	(3, 1.5 + 0.141120 + 0.2)
	(3.5, 1.5 - 0.350783 - 0.05) 
	(4, 1.5 - 0.756802 + 0.1)
	(4.5, 1.5 - 0.958851 - 0.2) 
	(5, 1.5 - 0.958851 + 0.05)
	(5.5, 1.5 - 0.598472 + 0.1) 
	(6, 1.5 + 0.141120 - 0.15)
	(6.5, 1.5 + 0.479425 + 0.05) 
	(7, 1.5 + 0.656986 - 0.05)
	(7.5, 1.5 + 0.989358 + 0.2) 
	(8, 1.5 + 0.989358 - 0.15)
	(8.5, 1.5 + 0.479425 + 0.1) 
	(9, 1.5 - 0.412118 - 0.1)
	(9.5, 1.5 - 0.801144 + 0.05)
};


		% Bandwidth around boundary point (x = 1)
		\draw[|-|, thick] (axis cs:0, 1.5) -- (axis cs:1,1.5);
		%\node[below] at (axis cs: 1, 0.6) {Bandwidth at boundary};
		
		% Bandwidth around interior point (x = 5)
		\draw[|-|, thick] (axis cs:2.75, 1.39) -- (axis cs:3.75, 1.39);
		%\node[below] at (axis cs: 5, 0.8) {Bandwidth at interior};
		\draw[-, thick, dashed, green] (axis cs:0, 1.5) -- (axis cs:1, 2.5);
		
		    \draw[-, thick, dashed, red] (axis cs:2.75, 1.89) -- (axis cs:3.75, 0.89);
		
		
	\end{axis}
\end{tikzpicture}
	\end{frame}
	
	\begin{frame}{Regressão linear local}
	\begin{itemize}
		\item O estimador de Nadaraya-Watson resolve:
	$$\hat{m}(x) = \operatorname{argmin}_{a \in \mathbb{R}} \sum_{i=1}^n K\left(\frac{X_i-x}{h}\right)(Y_i-a)^2$$
	\item Uma expansão de Taylor $m(X_i) \approx m(x) + m'(x)(X_i-x)$, e a expressão acima sugerem a estimação de $m(x)$ como o intercepto da {\color{blue}regressão linear local}:
	$$\min_{{\color{blue}a},b} \sum_{i=1}^n K\left(\frac{X_i-x}{h}\right)(Y_i-{\color{blue}a} - b(X_i-x))^2$$
	\item Coeficiente associado a $(X_i-x)$ , $\hat{b}$, estima $m'(x)$, embora esse estimador não seja ``bom'' (regressão quadrática local é preferível).
	
	\end{itemize}
\end{frame}
	
	\begin{frame}{Viés e variância da regressão linear local e ajuste automático à fronteira}
	\begin{itemize}
		\item Sob as mesmas condições do estimador de Nadaraya-Watson, o estimador $\tilde{m}(x)$ satisfaz, para os pontos interiores do suporte de $X$.
			$$\small \text{Vi\'{e}s}(\tilde{m}(x)) = \frac{1}{2} m^{\prime \prime}(x) \int_{-\infty}^{\infty} u^{2} K(u) d u h^{2}+ o(h^2)$$
	$$\mathbb{V}(\tilde{m}(x)) = \frac{\sigma^{2}(x)}{f(x) n h} \int_{-\infty}^{\infty} K^{2}(u) d u + o\left(\frac{1}{nh}\right)$$
	\item Ademais, uma fórmula análoga (com as mesmas taxas), vale para os pontos de fronteira do suporte \citep[Teorema 3.3]{Fan1996}.
	\item Banda ótima em termos de erro quadrático médio é dada por $h_n \propto n^{-1/5}$ 
	\end{itemize}
	\end{frame}
	
	\begin{frame}{Inferência}
	\begin{itemize}
		\item Sob condições de regularidade, temos que:
		
		$$\sqrt{nh}\left(\frac{\tilde{m}(x) - m(x)  - \text{Vi\'{e}s}(\tilde{m}(x)) }{\sqrt{\mathbb{V}[x]}}\right) \overset{d}{\to} \mathcal{N}(0,1) \, .$$
		\item Estimador é assintoticamente normal, embora não necessariamente seja centrado no parâmetro de interesse.
		\item Se $\sqrt{nh}\text{Vi\'{e}s}(\tilde{m}(x)) \to 0$, mau centramento desaparece assintoticamente, e intervalo de confiança $$[\tilde{m}(x) + z_{\alpha/2}\sqrt{\widehat{\mathbb{V}[\tilde{m}(x)]}}, \tilde{m}(x) + z_{1-\alpha/2}\sqrt{\widehat{\mathbb{V}[\tilde{m}(x)]}}]\,,$$ tem cobertura assintótica de $1-\alpha$.
		\begin{itemize}
			\item  $\sqrt{nh}\text{Vi\'{e}s}(\tilde{m}(x)) \to 0 \iff n h^5 \to 0$ (condição de \textit{undersmoothing}).
			\item Banda ótima para estimação pontual {\color{red}não} satisfaz essa condição.
			\item Condição também não é ``ótima'' sob diferentes critérios inferenciais \citep{armstrong2020simple,calonico2022coverage}
		\end{itemize}
	\end{itemize}
	\end{frame}
	\begin{frame}{Alternativas inferenciais}
	\begin{itemize}
		\item \citet{calonico2014robust}, \citet{calonico2018effect,calonico2022coverage}: estime $\text{Vi\'{e}s}(\tilde{m}(x))$ e ajuste erro padrão pela incerteza desta etapa, considerando o intervalo de confiança:
		 $$\tilde{m}(x) \pm z_{1- \alpha/2}\sqrt{\widehat{\mathbb{V}[\tilde{m}(x) +\text{Vi\'{e}s}(\tilde{m}(x))]}},\,,$$
		 \item \citet{armstrong2020simple}: considere o intervalo de confiança
		 	 $$\tilde{m}(x)- \widehat{\text{Vi\'{e}s}(\tilde{m}(x))} \pm {\color{red}c_{1- \alpha/2}(M)}\sqrt{\widehat{\mathbb{V}[\tilde{m}(x) +\text{Vi\'{e}s}(\tilde{m}(x))]}},\,,$$
		 	 onde $c_q(M)$ incorpora o efeito do pior viés possível, numa classe de funções ``suaves'', com parâmetro de suavidade $M$, sobre o valor crítico.
		 	 \begin{itemize}
		 	 	\item Intervalos \textit{undersmothened} ou corrigidos por viés podem ser vistos como escolhendo um $M$ implicitamente.
		 	 \end{itemize}
	\end{itemize}
	\end{frame}
	
	\begin{frame}{Aproximação uniforme}
		\begin{itemize}
			\item A discussão até aqui focou na qualidade da aproximação local em um único ponto.
			\item Entretanto, para o uso dos estimadores locais em análises posteriores, é interessante entender suas propriedades de aproximação, {\color{blue}uniformemente no suporte}.
			\item \citet{Einmahl2005}: para o estimador de Nadaraya-Watson com vetor aleatório $\mathbf{X}$ em $\mathbb{R}^d$,  e para qualquer subconjunto compacto $I$ em $\mathbb{R}^d$:
			
			$$\sup_{\boldsymbol{x} \in I}|\hat{m}(\boldsymbol{x}) - m(\boldsymbol{x}) - \operatorname{Viés}(\hat{m}(\boldsymbol{x}))| = O_{\mathbb{P}}\left(\sqrt{\frac{ (-\sum_{j=1}^d \log(h_j)) \lor \log \log n}{{nh_1\ldots h_d}}}\right)$$ 
			\item Resultados similares para dados com dependência temporal \citep{alvarez2023maximalinequalitylocalempirical}.
		\end{itemize}
	\end{frame}
	
	\begin{frame}{Aproximação ``global''}
		\begin{itemize}
			\item Recorde-se que, sob condições bastante gerais, o estimador de MQO de $Y$ num vetor $\boldsymbol{X}$ é consistente para o minimizador de
			
			$$\min_{b \in \mathbb{R}^d} \mathbb{E}[(Y-\boldsymbol{X}'b)^2]\,, $$
			e que esse mesmo minimizador também resolve
			
			$$\min_{b \in \mathbb{R}^d} \mathbb{E}[(\mathbb{E}[Y|\boldsymbol{X}]-\boldsymbol{X}'b)^2]\,, $$
			de modo que o estimador de MQO recupera a melhor aproximação linear à função de esperança condicional $\mathbb{E}[Y|\boldsymbol{X}]$ na norma $L_2(\mathbb{P}_{\boldsymbol{X}})$.
			\item No entanto, essa aproximação pode ser bastante ruim quando o objetivo não é ``resumir'' $\mathbb{E}[Y|\mathbf{X}]$, e sim analisar seu comportamento ``global''.
		\end{itemize}
	\end{frame}
	
	\begin{frame}{Aproximando a esperança condicional}
		\begin{itemize}
			\item Recorde-se que a função esperança condicional resolve:
			$$\min_{h \in L_2(\mathbb{P}_{\boldsymbol{X}})} \mathbb{E}[(Y-h(\boldsymbol{X}))^2]$$
			\item Quando o suporte de $\mathcal{X}$ de $\boldsymbol{X}$ é finito e pequeno relativamente a $n$, análogo amostral do problema acima é factível, visto que $L_2(\mathbb{P}_{\boldsymbol{X}}) = \{\sum_{s \in \mathcal{X}} a_s \mathbf{1}\{x = s\}: (a_s)_{s\in \mathcal{X}} \in \mathbb{R}^{\mathcal{X}}\}$.
			\begin{itemize}
				\item Regressão ``saturada'', com indicadores de todos os pontos do suporte, é um estimador não paramétrico factível de $\mathbb{E}[Y|\boldsymbol{X}]$.
			\end{itemize}
			\item No entanto, se $|\mathcal{X}|$ é grande em relação a $n$, análogo amostral torna-se impraticável.
			\begin{itemize}
				\item Pouco ou nenhum (se $|\mathcal{X}|\geq n$) grau de liberdade para estimar a esperança condicional.
				\item Em particular, se $\boldsymbol{X}$ contiver alguma variável aleatória contínua, existirão infinitas soluções que se adequarão aos dados perfeitamente, e poderão distar de $\mathbb{E}[Y|\boldsymbol{X}]$ tão mal quanto se queira.
			\end{itemize}
		\end{itemize}
	\end{frame}
	
	\begin{frame}{Estimador de séries}
		\begin{itemize}
			\item Dada a impraticabilidade de se estimar:
				$$\min_{h \in L_2(\mathbb{P}_{\boldsymbol{X}})} \sum_{i=1}^n (Y_i-h(\boldsymbol{X}_i))^2 \, ,$$
				o estimador não paramétrico de {\color{red}séries} propõe-se a estimar:
				
				$$\min_{h \in \Theta_{J_n}} \sum_{i=1}^n (Y_i-h(\boldsymbol{X}_i))^2 \, ,$$
				onde $(\Theta_{j})_{j \in \mathbb{N}}$ são uma sequência crescente de subespaços lineares de $L^2(\mathbb{P}_{\boldsymbol{X}})$ de {\color{blue}dimensão finita}, tais que a função de esperança condicional $m(\boldsymbol{X})$ satisfaz:
				
				$$m \in  \mathcal{A}\,,$$
				onde $(\mathcal{A},d)$ é um espaço métrico com $\cup_{i \in \mathbb{N}} \Theta_j \subseteq A \subseteq L^2(\mathbb{P}_{\boldsymbol{X}})$ e $A = \overline{\cup_{i \in \mathbb{N}} \Theta_j}$.
				\begin{itemize}
					\item Em palavras, $m$ é aproximada tão bem quanto se queira na métrica $d$, por um elemento de $\cup_{i \in \mathbb{N}} \Theta_j$.
				\end{itemize}
		\end{itemize}
	\end{frame}
	
	\begin{frame}{Viés de aproximação e \textit{trade-off} viés-variância}
	\begin{itemize}
		\item As propriedades de aproximação do estimador de séries dependem crucialmente do {\color{blue}viés de aproximação }:
		$$\rho_j = \inf_{s \in \Theta_j} d(m, s) \, .$$
		\item Essas quantidades são conhecidas teoricamente para diferentes classes de séries, com base em resultados de teoria da aproximação.
		\item A escolha de $J_n$ pode ser feita contrastando uma estimativa de $\hat{\rho}_j$ com a variância estimada do estimador, de modo a equalizar o trade-off viés-variância ou garantir inferência correta \citep{belloni2015some,Chen2024}.
	\end{itemize}
	\end{frame}
	
	\begin{frame}{Polinômios}
	\begin{itemize}
		\item Considere a situação em que o suporte de $\boldsymbol{X}$ é dado por um intervalo $[a,b]$ de $\mathbb{R}$.
		\item Nesse caso, podemos considerar a classe de polinômios:
		$$\Theta_j = \left\{ \sum_{l=0}^j a_l x^l: (a_l)_{l=0}^j \in \mathbb{R}^{j+1}\right\}\, ,$$
		\item Pelo teorema de Stone-Weierstrass, sabemos que, se $m$ é contínua em $[a,b]$, então $m$ é tão bem aproximada quanto se queira por um polinômio, na norma uniforme.
		\begin{itemize}
			\item Sob condições de diferenciabilidade e H\"{o}lder-continuidade nas derivadas de $m$, podemos encontrar uma função explícita para $\rho_j$ como função de $j$ e $m$.
		\end{itemize}
	\end{itemize}
	\end{frame}
	
	\begin{frame}{\textit{Splines}}
	\begin{itemize}
		\item Os polinômios da seção anterior podem funcionar bastante mal, especialmente na fronteira do suporte (fenômeno de Runge).
		\begin{itemize}
			\item Isso se deve à convexidade de $x^p$.
		\end{itemize}
		\item Uma alternativa bastante comum consiste no uso de \textit{splines}.
		\item Um \textit{spline} cúbico com \emph{nós} $ a < t_1 < t_2 < \ldots t_{j-4} < b$ é dado por:
		
		$$\Theta_j = \operatorname{span}\{1, x, x^2, x^3, \max(x-t_1,0)^3, \ldots,, \max(x-t_{j-4},0)^3\}$$
		\item Função $3-1=2$ vezes continuamente diferenciável em $[a,b]$.
		\item Ordem do polinômio é \emph{fixa}, o que mudam é o número de nós (uma convenção é usar percentis igualmente espaçados do suporte).
		\item Melhores propriedades de aproximação para funções em espaços H\"{o}lder que polinômios (veja \cite{belloni2015some}).
		\item Um \textit{B-Spline} é uma base do espaço gerado por um \textit{spline} com propriedades computacionais desejáveis. 
	\end{itemize}
	\end{frame}
	
	\begin{frame}{\textit{Tensor products}}
		\begin{itemize}
			\item Os resultados anteriores pressupunham que $\boldsymbol{X}$ era univariado.
			\item O caso multivariado pode ser lidado através de tensores. Construímos séries univariadas para cada uma das entradas de $\boldsymbol{X}$, e depois consideramos o espaço gerado pelos produtos das bases de cada um dos espaços.
			\item Por exemplo, se $\boldsymbol{X} = (X_1,X_2)$ é bidimensional, e consideramos polinômios de grau 2 para cada uma das dimensões, o espaço-produto é:
			$$\operatorname{span}\{1, x_1,x_2, x_1 x_2, x_1^2 x_2, x_2^2, x_1 x_2^2, x_1^2, x_2^2, x_1^2 x_2^2\}$$
			\item Resultado de aproximação uniforme para estimador de séries baseado em tensores de \textit{splines} \citep{belloni2015some} e escolha ``ótima'' de $J_n$:
			
			$$\sup _{x \in \mathcal{X}}|\widehat{h}(x)-m(x)| = O_{\mathbb{P}}\left(\left(\frac{\log n}{n}\right)^{s /(2 s+d)}\right)\, ,$$
			onde $s$ é o ``grau de suavidade'' de $m$, dado pelas restrições sobre sua derivada.
		\end{itemize}	\end{frame}
	\begin{frame}{Resultados adicionais de estimação de séries}
\begin{enumerate}
	\item Outras bases \citep[veja][]{chen2007,belloni2015some,cattaneo2020large}:
	\begin{itemize}
		\item Polinômios particionados.
		\item Bases de Fourier.
		\item Ondaletas.
	\end{itemize}
	\item Aproximações com dependência temporal \citep{Chen2015}.
	\item Outras funções objetivo.
	\begin{itemize}
		\item Quantil  \citep{belloni2019conditional}.
	\end{itemize}
\end{enumerate}
	\end{frame}
	\appendix
		\begin{frame}[allowframebreaks]{Bibliografia}
	\printbibliography

	\end{frame}

\end{document}

