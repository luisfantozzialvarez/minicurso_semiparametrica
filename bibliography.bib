@article{calonico2022coverage,
	title={Coverage error optimal confidence intervals for local polynomial regression},
	author={Calonico, Sebastian and Cattaneo, Matias D and Farrell, Max H},
	journal={Bernoulli},
	volume={28},
	number={4},
	year={2022}
}

@article{armstrong2020simple,
	title={Simple and honest confidence intervals in nonparametric regression},
	author={Armstrong, Timothy B and Koles{\'a}r, Michal},
	journal={Quantitative Economics},
	volume={11},
	number={1},
	pages={1--39},
	year={2020},
	publisher={Wiley Online Library}
}

@article{benhabib2018skewed,
	title={Skewed wealth distributions: Theory and empirics},
	author={Benhabib, Jess and Bisin, Alberto},
	journal={Journal of Economic Literature},
	volume={56},
	number={4},
	pages={1261--1291},
	year={2018},
	publisher={American Economic Association 2014 Broadway, Suite 305, Nashville, TN 37203-2425}
}

@article{achdou2022income,
	title={Income and wealth distribution in macroeconomics: A continuous-time approach},
	author={Achdou, Yves and Han, Jiequn and Lasry, Jean-Michel and Lions, Pierre-Louis and Moll, Benjamin},
	journal={The review of economic studies},
	volume={89},
	number={1},
	pages={45--86},
	year={2022},
	publisher={Oxford University Press}
}

@BOOK{Fan1996,
	title     = "Local polynomial modelling and its applications",
	author    = "Fan, Jianqing and Gijbels, Irene",
	publisher = "Chapman \& Hall/CRC",
	series    = "Chapman \& Hall/CRC Monographs on Statistics and Applied
	Probability",
	month     =  mar,
	year      =  1996,
	address   = "Philadelphia, PA",
	language  = "en"
}

@article{Newey1997,
	title={Convergence rates and asymptotic normality for series estimators},
	author={Newey, Whitney K},
	journal={Journal of econometrics},
	volume={79},
	number={1},
	pages={147--168},
	year={1997},
	publisher={Elsevier}
}

@article{cattaneo2020large,
	title={Large sample properties of partitioning-based series estimators},
	author={Cattaneo, Matias D and Farrell, Max H and Feng, Yingjie},
	journal={The Annals of Statistics},
	volume={48},
	number={3},
	pages={1718--1741},
	year={2020},
	publisher={JSTOR}
}

@incollection{chen2007,
	title = {Chapter 76 Large Sample Sieve Estimation of Semi-Nonparametric Models},
	editor = {James J. Heckman and Edward E. Leamer},
	series = {Handbook of Econometrics},
	publisher = {Elsevier},
	volume = {6},
	pages = {5549-5632},
	year = {2007},
	issn = {1573-4412},
	doi = {https://doi.org/10.1016/S1573-4412(07)06076-X},
	url = {https://www.sciencedirect.com/science/article/pii/S157344120706076X},
	author = {Xiaohong Chen},
	keywords = {sieve extremum estimation, series, sieve minimum distance, semiparametric two-step estimation, endogeneity in semi-nonparametric models},
	abstract = {Often researchers find parametric models restrictive and sensitive to deviations from the parametric specifications; semi-nonparametric models are more flexible and robust, but lead to other complications such as introducing infinite-dimensional parameter spaces that may not be compact and the optimization problem may no longer be well-posed. The method of sieves provides one way to tackle such difficulties by optimizing an empirical criterion over a sequence of approximating parameter spaces (i.e., sieves); the sieves are less complex but are dense in the original space and the resulting optimization problem becomes well-posed. With different choices of criteria and sieves, the method of sieves is very flexible in estimating complicated semi-nonparametric models with (or without) endogeneity and latent heterogeneity. It can easily incorporate prior information and constraints, often derived from economic theory, such as monotonicity, convexity, additivity, multiplicity, exclusion and nonnegativity. It can simultaneously estimate the parametric and nonparametric parts in semi-nonparametric models, typically with optimal convergence rates for both parts. This chapter describes estimation of semi-nonparametric econometric models via the method of sieves. We present some general results on the large sample properties of the sieve estimates, including consistency of the sieve extremum estimates, convergence rates of the sieve M-estimates, pointwise normality of series estimates of regression functions, root-n asymptotic normality and efficiency of sieve estimates of smooth functionals of infinite-dimensional parameters. Examples are used to illustrate the general results.}
}

@article{belloni2015some,
	title={Some new asymptotic theory for least squares series: Pointwise and uniform results},
	author={Belloni, Alexandre and Chernozhukov, Victor and Chetverikov, Denis and Kato, Kengo},
	journal={Journal of Econometrics},
	volume={186},
	number={2},
	pages={345--366},
	year={2015},
	publisher={Elsevier}
}

@article{calonico2018effect,
	title={On the effect of bias estimation on coverage accuracy in nonparametric inference},
	author={Calonico, Sebastian and Cattaneo, Matias D and Farrell, Max H},
	journal={Journal of the American Statistical Association},
	volume={113},
	number={522},
	pages={767--779},
	year={2018},
	publisher={Taylor \& Francis}
}

@article{buhlmann2007,
	author = {Peter B{\"u}hlmann and Torsten Hothorn},
	title = {{Boosting Algorithms: Regularization, Prediction and Model Fitting}},
	volume = {22},
	journal = {Statistical Science},
	number = {4},
	publisher = {Institute of Mathematical Statistics},
	pages = {477 -- 505},
	keywords = {generalized additive models, generalized linear models, gradient boosting, software, Survival analysis, Variable selection},
	year = {2007},
	doi = {10.1214/07-STS242},
	URL = {https://doi.org/10.1214/07-STS242}
}

@article{Bickel2009,
	author = {Peter J. Bickel and Ya’acov Ritov and Alexandre B. Tsybakov},
	title = {{Simultaneous analysis of Lasso and Dantzig selector}},
	volume = {37},
	journal = {The Annals of Statistics},
	number = {4},
	publisher = {Institute of Mathematical Statistics},
	pages = {1705 -- 1732},
	keywords = {linear models, Model selection, nonparametric statistics},
	year = {2009},
	doi = {10.1214/08-AOS620},
	URL = {https://doi.org/10.1214/08-AOS620}
}

@article{Belloni2013,
	author = {Alexandre Belloni and Victor Chernozhukov},
	title = {{Least squares after model selection in high-dimensional sparse models}},
	volume = {19},
	journal = {Bernoulli},
	number = {2},
	publisher = {Bernoulli Society for Mathematical Statistics and Probability},
	pages = {521 -- 547},
	keywords = {Lasso, OLS post-Lasso, post-model selection estimators},
	year = {2013},
	doi = {10.3150/11-BEJ410},
	URL = {https://doi.org/10.3150/11-BEJ410}
}

@article{Chernozhukov2018,
	author = {Chernozhukov, Victor and Chetverikov, Denis and Demirer, Mert and Duflo, Esther and Hansen, Christian and Newey, Whitney and Robins, James},
	title = "{Double/debiased machine learning for treatment and structural parameters}",
	journal = {The Econometrics Journal},
	volume = {21},
	number = {1},
	pages = {C1-C68},
	year = {2018},
	month = {01},
	abstract = "{We revisit the classic semi‐parametric problem of inference on a low‐dimensional parameter θ0 in the presence of high‐dimensional nuisance parameters η0. We depart from the classical setting by allowing for η0 to be so high‐dimensional that the traditional assumptions (e.g. Donsker properties) that limit complexity of the parameter space for this object break down. To estimate η0, we consider the use of statistical or machine learning (ML) methods, which are particularly well suited to estimation in modern, very high‐dimensional cases. ML methods perform well by employing regularization to reduce variance and trading off regularization bias with overfitting in practice. However, both regularization bias and overfitting in estimating η0 cause a heavy bias in estimators of θ0 that are obtained by naively plugging ML estimators of η0 into estimating equations for θ0. This bias results in the naive estimator failing to be N−1/2 consistent, where N is the sample size. We show that the impact of regularization bias and overfitting on estimation of the parameter of interest θ0 can be removed by using two simple, yet critical, ingredients: (1) using Neyman‐orthogonal moments/scores that have reduced sensitivity with respect to nuisance parameters to estimate θ0; (2) making use of cross‐fitting, which provides an efficient form of data‐splitting. We call the resulting set of methods double or debiased ML (DML). We verify that DML delivers point estimators that concentrate in an N−1/2‐neighbourhood of the true parameter values and are approximately unbiased and normally distributed, which allows construction of valid confidence statements. The generic statistical theory of DML is elementary and simultaneously relies on only weak theoretical requirements, which will admit the use of a broad array of modern ML methods for estimating the nuisance parameters, such as random forests, lasso, ridge, deep neural nets, boosted trees, and various hybrids and ensembles of these methods. We illustrate the general theory by applying it to provide theoretical properties of the following: DML applied to learn the main regression parameter in a partially linear regression model; DML applied to learn the coefficient on an endogenous variable in a partially linear instrumental variables model; DML applied to learn the average treatment effect and the average treatment effect on the treated under unconfoundedness; DML applied to learn the local average treatment effect in an instrumental variables setting. In addition to these theoretical applications, we also illustrate the use of DML in three empirical examples.}",
	issn = {1368-4221},
	doi = {10.1111/ectj.12097},
	url = {https://doi.org/10.1111/ectj.12097},
	eprint = {https://academic.oup.com/ectj/article-pdf/21/1/C1/27684918/ectj00c1.pdf},
}
@article{calonico2014robust,
	title={Robust nonparametric confidence intervals for regression-discontinuity designs},
	author={Calonico, Sebastian and Cattaneo, Matias D and Titiunik, Rocio},
	journal={Econometrica},
	volume={82},
	number={6},
	pages={2295--2326},
	year={2014},
	publisher={Wiley Online Library}
}

@misc{kennedy2023semiparametric,
	title={Semiparametric doubly robust targeted double machine learning: a review}, 
	author={Edward H. Kennedy},
	year={2023},
	eprint={2203.06469},
	archivePrefix={arXiv},
	primaryClass={stat.ME}
}

@article{Chernozhukov2022,
	author = {Chernozhukov, Victor and Escanciano, Juan Carlos and Ichimura, Hidehiko and Newey, Whitney K. and Robins, James M.},
	title = {Locally Robust Semiparametric Estimation},
	journal = {Econometrica},
	volume = {90},
	number = {4},
	pages = {1501-1535},
	keywords = {Local robustness, orthogonal moments, double robustness, semiparametric estimation, bias, GMM},
	doi = {https://doi.org/10.3982/ECTA16294},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.3982/ECTA16294},
	eprint = {https://onlinelibrary.wiley.com/doi/pdf/10.3982/ECTA16294},
	abstract = {Many economic and causal parameters depend on nonparametric or high dimensional first steps. We give a general construction of locally robust/orthogonal moment functions for GMM, where first steps have no effect, locally, on average moment functions. Using these orthogonal moments reduces model selection and regularization bias, as is important in many applications, especially for machine ic first steps. Also, associated standard errors are robust to misspecification when there is the same number of moment functions as parameters of interest. We use these orthogonal moments and cross-fitting to construct debiased machine learning estimators of functions of high dimensional conditional quantiles and of dynamic discrete choice parameters with high dimensional state variables. We show that additional first steps needed for the orthogonal moment functions have no effect, globally, on average orthogonal moment functions. We give a general approach to estimating those additional first steps. We characterize double robustness and give a variety of new doubly robust moment functions. We give general and simple regularity conditions for asymptotic theory.},
	year = {2022}
}

@article{belloni2019conditional,
	title={Conditional quantile processes based on series or many regressors},
	author={Belloni, Alexandre and Chernozhukov, Victor and Chetverikov, Denis and Fern{\'a}ndez-Val, Iv{\'a}n},
	journal={Journal of Econometrics},
	volume={213},
	number={1},
	pages={4--29},
	year={2019},
	publisher={Elsevier}
}

@article{Chen2015,
	title = {Optimal uniform convergence rates and asymptotic normality for series estimators under weak dependence and weak conditions},
	journal = {Journal of Econometrics},
	volume = {188},
	number = {2},
	pages = {447-465},
	year = {2015},
	note = {Heterogeneity in Panel Data and in Nonparametric Analysis in honor of Professor Cheng Hsiao},
	issn = {0304-4076},
	doi = {https://doi.org/10.1016/j.jeconom.2015.03.010},
	url = {https://www.sciencedirect.com/science/article/pii/S0304407615000792},
	author = {Xiaohong Chen and Timothy M. Christensen},
	keywords = {Nonparametric series regression, Optimal uniform convergence rates, Weak dependence, Random matrices, Splines, Wavelets, (Nonlinear) Irregular functionals, Sieve  statistics},
	abstract = {We show that spline and wavelet series regression estimators for weakly dependent regressors attain the optimal uniform (i.e. sup-norm) convergence rate (n/logn)−p/(2p+d) of Stone (1982), where d is the number of regressors and p is the smoothness of the regression function. The optimal rate is achieved even for heavy-tailed martingale difference errors with finite (2+(d/p))th absolute moment for d/p<2. We also establish the asymptotic normality of t statistics for possibly nonlinear, irregular functionals of the conditional mean function under weak conditions. The results are proved by deriving a new exponential inequality for sums of weakly dependent random matrices, which is of independent interest.}
}

@article{Ichimura2022,
	author = {Ichimura, Hidehiko and Newey, Whitney K.},
	title = {The influence function of semiparametric estimators},
	journal = {Quantitative Economics},
	volume = {13},
	number = {1},
	pages = {29-61},
	keywords = {Influence function, semiparametric estimation, NPIV, C13, C14, C20, C26, C36},
	doi = {https://doi.org/10.3982/QE826},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.3982/QE826},
	eprint = {https://onlinelibrary.wiley.com/doi/pdf/10.3982/QE826},
	abstract = {There are many economic parameters that depend on nonparametric first steps. Examples include games, dynamic discrete choice, average exact consumer surplus, and treatment effects. Often estimators of these parameters are asymptotically equivalent to a sample average of an object referred to as the influence function. The influence function is useful in local policy analysis, in evaluating local sensitivity of estimators, and constructing debiased machine learning estimators. We show that the influence function is a Gateaux derivative with respect to a smooth deviation evaluated at a point mass. This result generalizes the classic Von Mises (1947) and Hampel (1974) calculation to estimators that depend on smooth nonparametric first steps. We give explicit influence functions for first steps that satisfy exogenous or endogenous orthogonality conditions. We use these results to generalize the omitted variable bias formula for regression to policy analysis for and sensitivity to structural changes. We apply this analysis and find no sensitivity to endogeneity of average equivalent variation estimates in a gasoline demand application.},
	year = {2022}
}



@article{Singh2022,
	author = {Chernozhukov, V and Newey, W K and Singh, R},
	title = "{A simple and general debiased machine learning theorem with finite-sample guarantees}",
	journal = {Biometrika},
	volume = {110},
	number = {1},
	pages = {257-264},
	year = {2022},
	month = {06},
	abstract = "{Debiased machine learning is a meta-algorithm based on bias correction and sample splitting to calculate confidence intervals for functionals, i.e., scalar summaries, of machine learning algorithms. For example, an analyst may seek the confidence interval for a treatment effect estimated with a neural network. We present a non-asymptotic debiased machine learning theorem that encompasses any global or local functional of any machine learning algorithm that satisfies a few simple, interpretable conditions. Formally, we prove consistency, Gaussian approximation and semiparametric efficiency by finite-sample arguments. The rate of convergence is \\$n^\\{-1/2\\}\\$ for global functionals, and it degrades gracefully for local functionals. Our results culminate in a simple set of conditions that an analyst can use to translate modern learning theory rates into traditional statistical inference. The conditions reveal a general double robustness property for ill-posed inverse problems.}",
	issn = {1464-3510},
	doi = {10.1093/biomet/asac033},
	url = {https://doi.org/10.1093/biomet/asac033},
	eprint = {https://academic.oup.com/biomet/article-pdf/110/1/257/49160070/asac033.pdf},
}



@misc{wager2016adaptive,
	title={Adaptive Concentration of Regression Trees, with Application to Random Forests}, 
	author={Stefan Wager and Guenther Walther},
	year={2016},
	eprint={1503.06388},
	archivePrefix={arXiv},
	primaryClass={math.ST}
}

@ARTICLE{Chen1999,
	
	author={Xiaohong Chen and White, H.},
	
	journal={IEEE Transactions on Information Theory}, 
	
	title={Improved rates and asymptotic normality for nonparametric neural network estimators}, 
	
	year={1999},
	
	volume={45},
	
	number={2},
	
	pages={682-691},
	
	keywords={Neural networks;Artificial neural networks;Kernel;Feedforward neural networks;Associate members;Convergence;Finance;Statistics;Gaussian distribution},
	
	doi={10.1109/18.749011}}

@article{Farrell2021,
	author = {Farrell, Max H. and Liang, Tengyuan and Misra, Sanjog},
	title = {Deep Neural Networks for Estimation and Inference},
	journal = {Econometrica},
	volume = {89},
	number = {1},
	pages = {181-213},
	keywords = {Deep learning, neural networks, rectified linear unit, nonasymptotic bounds, convergence rates, semiparametric inference, treatment effects, program evaluation},
	doi = {https://doi.org/10.3982/ECTA16901},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.3982/ECTA16901},
	eprint = {https://onlinelibrary.wiley.com/doi/pdf/10.3982/ECTA16901},
	abstract = {We study deep neural networks and their use in semiparametric inference. We establish novel nonasymptotic high probability bounds for deep feedforward neural nets. These deliver rates of convergence that are sufficiently fast (in some cases minimax optimal) to allow us to establish valid second-step inference after first-step estimation with deep learning, a result also new to the literature. Our nonasymptotic high probability bounds, and the subsequent semiparametric inference, treat the current standard architecture: fully connected feedforward neural networks (multilayer perceptrons), with the now-common rectified linear unit activation function, unbounded weights, and a depth explicitly diverging with the sample size. We discuss other architectures as well, including fixed-width, very deep networks. We establish the nonasymptotic bounds for these deep nets for a general class of nonparametric regression-type loss functions, which includes as special cases least squares, logistic regression, and other generalized linear models. We then apply our theory to develop semiparametric inference, focusing on causal parameters for concreteness, and demonstrate the effectiveness of deep learning with an empirical application to direct mail marketing.},
	year = {2021}
}

@misc{syrgkanis2020estimation,
	title={Estimation and Inference with Trees and Forests in High Dimensions}, 
	author={Vasilis Syrgkanis and Manolis Zampetakis},
	year={2020},
	eprint={2007.03210},
	archivePrefix={arXiv},
	primaryClass={math.ST}
}

@misc{chetverikov2023selecting,
	title={Selecting Penalty Parameters of High-Dimensional M-Estimators using Bootstrapping after Cross-Validation}, 
	author={Denis Chetverikov and Jesper Riis-Vestergaard Sørensen},
	year={2023},
	eprint={2104.04716},
	archivePrefix={arXiv},
	primaryClass={math.ST}
}
@misc{chetverikov2022weightedaverage,
	title={Weighted-average quantile regression}, 
	author={Denis Chetverikov and Yukun Liu and Aleh Tsyvinski},
	year={2022},
	eprint={2203.03032},
	archivePrefix={arXiv},
	primaryClass={econ.EM}
}
@article{Chen2023,
	title = {Efficient estimation of average derivatives in NPIV models: Simulation comparisons of neural network estimators},
	journal = {Journal of Econometrics},
	volume = {235},
	number = {2},
	pages = {1848-1875},
	year = {2023},
	issn = {0304-4076},
	doi = {https://doi.org/10.1016/j.jeconom.2022.12.014},
	url = {https://www.sciencedirect.com/science/article/pii/S0304407623000349},
	author = {Jiafeng Chen and Xiaohong Chen and Elie Tamer},
	keywords = {Artificial Neural Networks, Relu, Sigmoid, Nonparametric instrumental variables, Weighted average derivatives, Optimal sieve minimum distance, Efficient influence, Semiparametric efficiency, Endogenous demand},
	abstract = {Artificial Neural Networks (ANNs) can be viewed as nonlinear sieves that can approximate complex functions of high dimensional variables more effectively than linear sieves. We investigate the performance of various ANNs in nonparametric instrumental variables (NPIV) models of moderately high dimensional covariates that are relevant to empirical economics. We present two efficient procedures for estimation and inference on a weighted average derivative (WAD): an orthogonalized plug-in with optimally-weighted sieve minimum distance (OP-OSMD) procedure and a sieve efficient score (ES) procedure. Both estimators for WAD use ANN sieves to approximate the unknown NPIV function and are n-asymptotically normal and first-order equivalent. We provide a detailed practitioner’s recipe for implementing both efficient procedures. We compare their finite-sample performances in various simulation designs that involve smooth NPIV function of up to 13 continuous covariates, different nonlinearities and covariate correlations. Some Monte Carlo findings include: (1) tuning and optimization are more delicate in ANN estimation; (2) given proper tuning, both ANN estimators with various architectures can perform well; (3) easier to tune ANN OP-OSMD estimators than ANN ES estimators; (4) stable inferences are more difficult to achieve with ANN (than spline) estimators; (5) there are gaps between current implementations and approximation theories. Finally, we apply ANN NPIV to estimate average partial derivatives in two empirical demand examples with multivariate covariates.}
}

@article{Mcfadden1989,
	ISSN = {00129682, 14680262},
	URL = {http://www.jstor.org/stable/1913621},
	abstract = {This paper proposes a simple modification of a conventional method of moments estimator for a discrete response model, replacing response probabilities that require numerical integration with estimators obtained by Monte Carlo simulation. This method of simulated moments (MSM) does not require precise estimates of these probabilities for consistency and asymptotic normality, relying instead on the law of large numbers operating across observations to control simulation error, and hence can use simulations of practical size. The method is useful for models such as high-dimensional multinomial probit (MNP), where computation has restricted applications.},
	author = {Daniel McFadden},
	journal = {Econometrica},
	number = {5},
	pages = {995--1026},
	publisher = {[Wiley, Econometric Society]},
	title = {A Method of Simulated Moments for Estimation of Discrete Response Models Without Numerical Integration},
	urldate = {2024-03-12},
	volume = {57},
	year = {1989}
}

@article{Kaji2023,
	author = {Kaji, Tetsuya and Manresa, Elena and Pouliot, Guillaume},
	title = {An Adversarial Approach to Structural Estimation},
	journal = {Econometrica},
	volume = {91},
	number = {6},
	pages = {2041-2063},
	keywords = {Structural estimation, generative adversarial networks, neural networks, simulation-based estimation, efficient estimation},
	doi = {https://doi.org/10.3982/ECTA18707},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.3982/ECTA18707},
	eprint = {https://onlinelibrary.wiley.com/doi/pdf/10.3982/ECTA18707},
	abstract = {We propose a new simulation-based estimation method, adversarial estimation, for structural models. The estimator is formulated as the solution to a minimax problem between a generator (which generates simulated observations using the structural model) and a discriminator (which classifies whether an observation is simulated). The discriminator maximizes the accuracy of its classification while the generator minimizes it. We show that, with a sufficiently rich discriminator, the adversarial estimator attains parametric efficiency under correct specification and the parametric rate under misspecification. We advocate the use of a neural network as a discriminator that can exploit adaptivity properties and attain fast rates of convergence.},
	year = {2023}
}

@article{Kaji2021,
	Author = {Kaji, Tetsuya and Manresa, Elena and Pouliot, Guillaume A.},
	Title = {Adversarial Inference Is Efficient},
	Journal = {AEA Papers and Proceedings},
	Volume = {111},
	Year = {2021},
	Month = {May},
	Pages = {621-25},
	DOI = {10.1257/pandp.20211037},
	URL = {https://www.aeaweb.org/articles?id=10.1257/pandp.20211037}}


@misc{alvarez2023maximalinequalitylocalempirical,
	title={A maximal inequality for local empirical processes under weak dependence}, 
	author={Luis Alvarez and Cristine Pinto},
	year={2023},
	eprint={2307.01328},
	archivePrefix={arXiv},
	primaryClass={econ.EM},
	url={https://arxiv.org/abs/2307.01328}, 
}

@article{Einmahl2005,
	author = {Uwe Einmahl and David M. Mason},
	title = {{Uniform in bandwidth consistency of kernel-type function estimators}},
	volume = {33},
	journal = {The Annals of Statistics},
	number = {3},
	publisher = {Institute of Mathematical Statistics},
	pages = {1380 -- 1403},
	keywords = {consistency, Kernel-type function estimator, uniform in bandwidth},
	year = {2005},
	doi = {10.1214/009053605000000129},
	URL = {https://doi.org/10.1214/009053605000000129}
}


@misc{chen2024adaptiveestimationuniformconfidence,
	title={Adaptive Estimation and Uniform Confidence Bands for Nonparametric Structural Functions and Elasticities}, 
	author={Xiaohong Chen and Timothy Christensen and Sid Kankanala},
	year={2024},
	eprint={2107.11869},
	archivePrefix={arXiv},
	primaryClass={econ.EM},
	url={https://arxiv.org/abs/2107.11869}, 
}

@article{Chen2024,
	author = {Chen, Xiaohong and Christensen, Timothy and Kankanala, Sid},
	title = "{Adaptive Estimation and Uniform Confidence Bands for Nonparametric Structural Functions and Elasticities}",
	journal = {The Review of Economic Studies},
	pages = {rdae025},
	year = {2024},
	month = {03},
	abstract = "{We introduce two data-driven procedures for optimal estimation and inference in nonparametric models using instrumental variables. The first is a data-driven choice of sieve dimension for a popular class of sieve two-stage least-squares estimators. When implemented with this choice, estimators of both the structural function h0 and its derivatives (such as elasticities) converge at the fastest possible (i.e. minimax) rates in sup-norm. The second is for constructing uniform confidence bands (UCBs) for h0 and its derivatives. Our UCBs guarantee coverage over a generic class of data-generating processes and contract at the minimax rate, possibly up to a logarithmic factor. As such, our UCBs are asymptotically more efficient than UCBs based on the usual approach of undersmoothing. As an application, we estimate the elasticity of the intensive margin of firm exports in a monopolistic competition model of international trade. Simulations illustrate the good performance of our procedures in empirically calibrated designs. Our results provide evidence against common parameterizations of the distribution of unobserved firm heterogeneity.}",
	issn = {0034-6527},
	doi = {10.1093/restud/rdae025},
	url = {https://doi.org/10.1093/restud/rdae025},
	eprint = {https://academic.oup.com/restud/advance-article-pdf/doi/10.1093/restud/rdae025/57021176/rdae025.pdf},
}



